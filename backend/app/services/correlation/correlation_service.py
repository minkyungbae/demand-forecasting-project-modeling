from typing import Dict, List, Optional
from datetime import datetime
import time
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
from scipy.stats import chi2_contingency
from app.core.database import get_database
from app.models.correlation import CorrelationAnalysisResponse, TopCorrelationItem
from app.services.correlation.weight_calculator import WeightCalculator
from app.services.correlation.correlation_repository import CorrelationRepository
from app.services.file.file_repository import FileRepository
from app.services.file.file_service import FileService
from app.services.weight.weight_repository import WeightRepository
from app.services.file.file_analysis_config_repository import FileAnalysisConfigRepository
from app.services.analysis.analysis_repository import AnalysisRepository

class CorrelationService:
    """ìƒê´€ê´€ê³„ ë¶„ì„ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.weight_calculator = WeightCalculator()
        self.repository = CorrelationRepository()
        self.file_repository = FileRepository()
        self.file_service = FileService()
        self.weight_repository = WeightRepository()
        self.config_repository = FileAnalysisConfigRepository()
        self.analysis_repository = AnalysisRepository()
    
    async def analyze_correlations(
        self, 
        file_id: str,
        features: Optional[List[str]],
        user_id: str
    ) -> CorrelationAnalysisResponse:
        """ìƒê´€ê´€ê³„ ë¶„ì„ ë° ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        # 1. íŒŒì¼ ì†Œìœ ê¶Œ í™•ì¸ ë° target_column ê°€ì ¸ì˜¤ê¸°
        file_info = await self.file_repository.get_sales_info(file_id, user_id)
        if not file_info:
            raise ValueError("íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        target_column = file_info.get('target_column')
        if not target_column:
            raise ValueError("íŒŒì¼ ì—…ë¡œë“œ ì‹œ target_columnì„ ì§€ì •í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. íŒŒì¼ì„ ë‹¤ì‹œ ì—…ë¡œë“œí•˜ê±°ë‚˜ target_columnì„ ì§€ì •í•´ì£¼ì„¸ìš”.")
        
        # 2. ì„¤ì • ì •ë³´ ê°€ì ¸ì˜¤ê¸° (valid_columnsì™€ grouping_columns ì‚¬ìš©)
        config = await self.file_service.config_repository.get_config(file_id, target_column)
        group_by_columns = config.get('group_by_columns', []) if config else []
        primary_group_by_column = config.get('group_by_column') if config else None
        date_column = config.get('date_column') if config else None
        lag_feature_columns = config.get('lag_feature_columns', []) if config else []
        
        # valid_columnsê°€ ìˆìœ¼ë©´ ì‚¬ìš© (Lag í”¼ì²˜ í¬í•¨ëœ ìœ íš¨ ì»¬ëŸ¼), ì—†ìœ¼ë©´ features ì‚¬ìš© (í•˜ìœ„ í˜¸í™˜ì„±)
        valid_columns = config.get('valid_columns', []) if config else []
        if not features or len(features) == 0:
            # featuresê°€ ì—†ìœ¼ë©´ valid_columns ì‚¬ìš©
            if valid_columns and len(valid_columns) > 0:
                # valid_columnsëŠ” ê·¸ë£¹í™” ì»¬ëŸ¼ì„ ì œì™¸í•œ ìˆœìˆ˜ í”¼ì²˜ë§Œ í¬í•¨
                # grouping_columnsëŠ” ìƒê´€ê´€ê³„ ë¶„ì„ì—ì„œ ê·¸ë£¹í™” ìš©ë„ë¡œë§Œ ì‚¬ìš©
                features_for_correlation = [col for col in valid_columns if col not in (group_by_columns or [])]
                if features_for_correlation:
                    features = features_for_correlation
                    print(f"âœ… valid_columns ì‚¬ìš©: {len(features)}ê°œ í”¼ì²˜ (Lag í”¼ì²˜ í¬í•¨)")
            else:
                raise ValueError("featuresë¥¼ ì§€ì •í•˜ê±°ë‚˜, íŒŒì¼ ì—…ë¡œë“œ ì‹œ target_columnì„ ì§€ì •í•˜ì—¬ ì»¬ëŸ¼ ì¶”ì²œì„ ë¨¼ì € ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤.")
        
        # 3. ë°ì´í„° ë¡œë“œ ë° Lag í”¼ì²˜ ìƒì„± (í•„ìš”ì‹œ)
        data = await self._load_data(file_id)
        
        # Lag í”¼ì²˜ê°€ í•„ìš”í•œë° ë°ì´í„°ì— ì—†ìœ¼ë©´ ì‹¤ì‹œê°„ ìƒì„±
        if lag_feature_columns and date_column:
            from app.services.feature.lag_feature_generator import LagFeatureGenerator
            lag_generator = LagFeatureGenerator()
            
            # Lag í”¼ì²˜ê°€ ë°ì´í„°ì— ìˆëŠ”ì§€ í™•ì¸
            sample_row = data[0] if data else {}
            needs_lag_generation = any(lag_col not in sample_row for lag_col in lag_feature_columns[:3])  # ì²˜ìŒ 3ê°œë§Œ ì²´í¬
            
            if needs_lag_generation:
                print(f"ğŸ“Š Lag í”¼ì²˜ ì‹¤ì‹œê°„ ìƒì„± ì¤‘...")
                try:
                    # Lag í”¼ì²˜ ìƒì„±ì— í•„ìš”í•œ ì •ë³´
                    valid_base_columns = [col for col in valid_columns if not col.endswith('_lag_7d') and not col.endswith('_lag_14d') and not col.endswith('_lag_30d')]
                    grouping_cols = config.get('grouping_columns', []) if config else []
                    
                    processed_df, _ = await lag_generator.generate_lag_features(
                        data=data,
                        date_column=date_column,
                        target_column=target_column,
                        numeric_columns=valid_base_columns,
                        group_by_columns=grouping_cols,
                        lag_periods=[7, 30]
                    )
                    
                    # DataFrameì„ ë‹¤ì‹œ List[Dict]ë¡œ ë³€í™˜
                    data = processed_df.to_dict('records')
                    print(f"âœ… Lag í”¼ì²˜ ìƒì„± ì™„ë£Œ: {len(lag_feature_columns)}ê°œ ì»¬ëŸ¼")
                except Exception as e:
                    print(f"âš ï¸ Lag í”¼ì²˜ ìƒì„± ì‹¤íŒ¨: {str(e)}, ê¸°ì¡´ ë°ì´í„° ì‚¬ìš©")
        
        # 4. ìƒê´€ê³„ìˆ˜ ê³„ì‚° (ì „ì²´ + ê·¸ë£¹ë³„)
        # 4-1. ì „ì²´ ìƒê´€ê³„ìˆ˜ (ê·¸ë£¹í™” ì—†ì´) - valid_columnsë§Œ ì‚¬ìš© (ê·¸ë£¹í™” ì»¬ëŸ¼ ì œì™¸)
        overall_correlations = await self._calculate_correlations(
            data, target_column, features, None
        )
        
        # 4-2. ê·¸ë£¹ë³„ ìƒê´€ê³„ìˆ˜ ê³„ì‚°
        group_correlations_dict = {}
        if group_by_columns:
            df_check = pd.DataFrame(data)
            for group_col in group_by_columns:
                if group_col in df_check.columns:
                    print(f"ğŸ“Š ê·¸ë£¹ë³„ ìƒê´€ê³„ìˆ˜ ê³„ì‚° ì‹œì‘: '{group_col}'")
                    group_corr = await self._calculate_correlations_by_group(
                        data, target_column, features, group_col
                    )
                    if group_corr:
                        group_correlations_dict[f"by_{group_col}"] = group_corr
        
        # ì „ì²´ ìƒê´€ê³„ìˆ˜ë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš© (í•˜ìœ„ í˜¸í™˜ì„±)
        correlations = overall_correlations
        
        # 5. ê°€ì¤‘ì¹˜ ê³„ì‚°
        start_time = time.time()
        weights = self.weight_calculator.calculate(correlations)
        
        # 6. ì‹œê°í™” ìƒì„±
        chart = await self._create_chart(correlations, target_column)
        
        processing_time = time.time() - start_time
        
        # 7. Feature Weights ì €ì¥
        await self.weight_repository.save_weights(
            file_id=file_id,
            user_id=user_id,
            weights=weights
        )
        
        # 8. Analysis Results ì €ì¥
        analysis_result = await self.analysis_repository.save_analysis_result(
            file_id=file_id,
            user_id=user_id,
            analysis_type='correlation',
            metrics={},  # ìƒê´€ê´€ê³„ ë¶„ì„ì€ ë³„ë„ ë©”íŠ¸ë¦­ ì—†ìŒ
            feature_count=len(features),
            target_column=target_column,
            group_by=[],
            processing_time_seconds=processing_time,
            result={
                'correlation_matrix': correlations,
                'chart': chart
            }
        )
        
        # 9. ê¸°ì¡´ ë°©ì‹ í˜¸í™˜ì„±ì„ ìœ„í•œ ì €ì¥ (ì„ íƒì )
        result = await self.repository.save(
            file_id=file_id,
            user_id=user_id,
            target_column=target_column,
            correlations=correlations,
            weights=weights,
            chart=chart
        )
        
        # ìƒê´€ê´€ê³„ í–‰ë ¬ êµ¬ì„± (ì „ì²´ + ê·¸ë£¹ë³„)
        correlation_matrix = {
            'overall': correlations,  # ì „ì²´ ìƒê´€ê³„ìˆ˜
            **group_correlations_dict  # ê·¸ë£¹ë³„ ìƒê´€ê³„ìˆ˜ (ì˜ˆ: {"by_ìƒí’ˆ_ID": {...}, "by_ë¸Œëœë“œ": {...}})
        }
        
        return CorrelationAnalysisResponse(
            correlation_matrix=correlation_matrix,
            top_correlations=self._get_top_correlations(correlations),
            chart=chart,
            weights=weights,
            correlation_id=result['correlation_id'],
            created_at=result['created_at']
        )
    
    async def get_correlations(self, file_id: str) -> Optional[CorrelationAnalysisResponse]:
        """ì €ì¥ëœ ìƒê´€ê´€ê³„ ë¶„ì„ ê²°ê³¼ ì¡°íšŒ"""
        result = await self.repository.get_by_file_id(file_id)
        if not result:
            return None
        
        # correlation_matrixê°€ ì´ë¯¸ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš°ì™€ ë¬¸ìì—´ í‚¤ì¸ ê²½ìš° ì²˜ë¦¬
        correlation_matrix = result.get('correlation_matrix', {})
        if isinstance(correlation_matrix, dict) and 'overall' in correlation_matrix:
            # ì´ë¯¸ ì˜¬ë°”ë¥¸ í˜•ì‹
            matrix = correlation_matrix
        else:
            # ê¸°ì¡´ í˜•ì‹: target_columnì„ í‚¤ë¡œ ì‚¬ìš©
            matrix = {'overall': correlation_matrix}
        
        return CorrelationAnalysisResponse(
            correlation_matrix=matrix,
            top_correlations=self._get_top_correlations(matrix.get('overall', {})),
            chart=result.get('chart', ''),
            weights=result.get('weights', {}),
            correlation_id=result.get('correlation_id', ''),
            created_at=result.get('created_at', datetime.now())
        )
    
    async def _load_data(self, file_id: str) -> List[dict]:
        """MongoDBì—ì„œ ë°ì´í„° ë¡œë“œ"""
        data = await self.file_repository.get_csv_data(file_id, 0, 10000)
        # CSV Collectionì—ì„œ ê°€ì ¸ì˜¨ ë°ì´í„°ëŠ” data í•„ë“œ ì•ˆì— ìˆìŒ
        if data and 'data' in data[0]:
            # data í•„ë“œë¥¼ í¼ì³ì„œ ì‚¬ìš©
            return [row['data'] for row in data]
        return data
    
    def _detect_group_column(self, data: List[dict], target_column: str, features: List[str]) -> Optional[str]:
        """ì œí’ˆë³„ ê·¸ë£¹í™” ì»¬ëŸ¼ ìë™ ê°ì§€ (ìƒí’ˆ_ID, ìƒí’ˆëª… ë“±)"""
        if not data:
            return None
        
        df = pd.DataFrame(data)
        # ì œí’ˆ ì‹ë³„ ê°€ëŠ¥í•œ ì»¬ëŸ¼ í‚¤ì›Œë“œ
        group_keywords = ['ìƒí’ˆ', 'ì œí’ˆ', 'product', 'item', 'id', 'ID', 'ëª…']
        
        # íƒ€ê²Ÿ ì»¬ëŸ¼ê³¼ featuresê°€ ì•„ë‹Œ ì»¬ëŸ¼ ì¤‘ì—ì„œ ê·¸ë£¹í™” ì»¬ëŸ¼ ì°¾ê¸°
        all_columns = set(df.columns)
        exclude_columns = {target_column} | set(features)
        candidate_columns = all_columns - exclude_columns
        
        for col in candidate_columns:
            col_lower = col.lower()
            # í‚¤ì›Œë“œ ë§¤ì¹­
            if any(keyword in col_lower for keyword in group_keywords):
                # ê³ ìœ  ê°’ì´ ì—¬ëŸ¬ ê°œì¸ì§€ í™•ì¸ (ê·¸ë£¹í™” ê°€ëŠ¥)
                unique_count = df[col].nunique()
                total_count = len(df)
                # ê³ ìœ  ê°’ì´ ë§ì§€ë§Œ ì „ì²´ì˜ ì¼ë¶€ì¸ ê²½ìš° ê·¸ë£¹í™” ì»¬ëŸ¼ì¼ ê°€ëŠ¥ì„±
                if 1 < unique_count < total_count * 0.5:  # ì „ì²´ì˜ 50% ë¯¸ë§Œì´ë©´ ê·¸ë£¹í™” ì»¬ëŸ¼ì¼ ê°€ëŠ¥ì„±
                    print(f"ğŸ“Š ì œí’ˆë³„ ê·¸ë£¹í™” ì»¬ëŸ¼ ìë™ ê°ì§€: '{col}' ({unique_count}ê°œ ê³ ìœ  ê°’)")
                    return col
        
        return None
    
    async def _calculate_correlations(
        self, 
        data: List[dict], 
        target: str, 
        features: List[str],
        group_by_column: Optional[str] = None
    ) -> Dict[str, float]:
        """ìƒê´€ê³„ìˆ˜ ê³„ì‚° (object íƒ€ì… ì§€ì›: ì›í•« ì¸ì½”ë”©, ë‚ ì§œëŠ” ê·¸ë£¹ë³„ ì¸ë±ìŠ¤)
        
        Args:
            data: ë¶„ì„í•  ë°ì´í„°
            target: íƒ€ê²Ÿ ì»¬ëŸ¼ëª…
            features: í”¼ì²˜ ì»¬ëŸ¼ ëª©ë¡
            group_by_column: ê·¸ë£¹í™”í•  ì»¬ëŸ¼ëª… (ì˜ˆ: ìƒí’ˆ_ID, ìƒí’ˆëª…). ìˆìœ¼ë©´ ë‚ ì§œë¥¼ ê·¸ë£¹ë³„ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
        """
        df = pd.DataFrame(data)
        correlations = {}
        
        if target not in df.columns:
            raise ValueError(f"íƒ€ê²Ÿ ì»¬ëŸ¼ {target}ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ê·¸ë£¹í™” ê¸°ì¤€ ì»¬ëŸ¼ í™•ì¸
        group_by_series = None
        if group_by_column and group_by_column in df.columns:
            group_by_series = df[group_by_column]
            print(f"ğŸ“Š ê·¸ë£¹í™” ê¸°ì¤€: '{group_by_column}' - ë‚ ì§œë¥¼ ê·¸ë£¹ë³„ ì¸ë±ìŠ¤ë¡œ ë³€í™˜í•©ë‹ˆë‹¤")
        
        # íƒ€ê²Ÿ ì»¬ëŸ¼ ì „ì²˜ë¦¬
        target_series = self._preprocess_column(df[target], group_by_series)
        
        for feature in features:
            if feature not in df.columns:
                continue
            
            try:
                # í”¼ì²˜ ì»¬ëŸ¼ ì „ì²˜ë¦¬ (ê°™ì€ ê·¸ë£¹í™” ê¸°ì¤€ ì‚¬ìš©)
                feature_series = self._preprocess_column(df[feature], group_by_series)
                
                # ìƒê´€ê³„ìˆ˜ ê³„ì‚°
                if pd.api.types.is_numeric_dtype(target_series) and pd.api.types.is_numeric_dtype(feature_series):
                    # ë‘˜ ë‹¤ ìˆ«ìí˜•: ì •ê·œí™” í›„ í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜
                    # NaN ê°’ ì œê±°
                    valid_mask = ~(target_series.isna() | feature_series.isna())
                    if valid_mask.sum() < 2:  # ìµœì†Œ 2ê°œ ì´ìƒì˜ ìœ íš¨í•œ ë°ì´í„° í•„ìš”
                        continue
                    
                    target_valid = target_series[valid_mask]
                    feature_valid = feature_series[valid_mask]
                    
                    # ì •ê·œí™” (StandardScaler: í‰ê·  0, í‘œì¤€í¸ì°¨ 1)
                    scaler = StandardScaler()
                    # 2D ë°°ì—´ë¡œ ë³€í™˜ (sklearn ìš”êµ¬ì‚¬í•­)
                    target_normalized = scaler.fit_transform(target_valid.values.reshape(-1, 1)).flatten()
                    feature_normalized = scaler.fit_transform(feature_valid.values.reshape(-1, 1)).flatten()
                    
                    # ì •ê·œí™”ëœ ë°ì´í„°ë¡œ ìƒê´€ê³„ìˆ˜ ê³„ì‚°
                    corr = np.corrcoef(target_normalized, feature_normalized)[0, 1]
                elif pd.api.types.is_numeric_dtype(target_series) or pd.api.types.is_numeric_dtype(feature_series):
                    # í•˜ë‚˜ëŠ” ìˆ«ìí˜•, í•˜ë‚˜ëŠ” ë²”ì£¼í˜•: ì›í•« ì¸ì½”ë”© í›„ ìƒê´€ê³„ìˆ˜
                    corr = self._calculate_categorical_correlation(target_series, feature_series)
                else:
                    # ë‘˜ ë‹¤ ë²”ì£¼í˜•: CramÃ©r's V (ë˜ëŠ” ì›í•« ì¸ì½”ë”© í›„ ìƒê´€ê³„ìˆ˜)
                    corr = self._calculate_categorical_correlation(target_series, feature_series)
                
                if not np.isnan(corr) and not pd.isna(corr):
                    correlations[feature] = float(corr)
            except Exception as e:
                # ìƒê´€ê³„ìˆ˜ ê³„ì‚° ì‹¤íŒ¨ ì‹œ í•´ë‹¹ í”¼ì²˜ëŠ” ì œì™¸
                print(f"âš ï¸ í”¼ì²˜ '{feature}'ì˜ ìƒê´€ê³„ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {str(e)}")
                continue
        
        return correlations
    
    async def _calculate_correlations_by_group(
        self,
        data: List[dict],
        target: str,
        features: List[str],
        group_by_column: str
    ) -> Dict[str, Dict[str, float]]:
        """ê·¸ë£¹ë³„ ìƒê´€ê³„ìˆ˜ ê³„ì‚°
        
        Args:
            data: ë¶„ì„í•  ë°ì´í„°
            target: íƒ€ê²Ÿ ì»¬ëŸ¼ëª…
            features: í”¼ì²˜ ì»¬ëŸ¼ ëª©ë¡
            group_by_column: ê·¸ë£¹í™”í•  ì»¬ëŸ¼ëª…
        
        Returns:
            {group_value: {feature: correlation}} í˜•ì‹ì˜ ë”•ì…”ë„ˆë¦¬
        """
        df = pd.DataFrame(data)
        
        if group_by_column not in df.columns:
            return {}
        
        group_correlations = {}
        group_values = df[group_by_column].unique()
        
        for group_value in group_values:
            group_df = df[df[group_by_column] == group_value]
            if len(group_df) < 3:  # ë°ì´í„°ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ìŠ¤í‚µ
                continue
            
            group_corr = {}
            target_series = self._preprocess_column(group_df[target], None)
            
            for feature in features:
                if feature not in group_df.columns:
                    continue
                
                try:
                    feature_series = self._preprocess_column(group_df[feature], None)
                    
                    if pd.api.types.is_numeric_dtype(target_series) and pd.api.types.is_numeric_dtype(feature_series):
                        # ì •ê·œí™” í›„ í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜
                        valid_mask = ~(target_series.isna() | feature_series.isna())
                        if valid_mask.sum() < 2:
                            continue
                        
                        target_valid = target_series[valid_mask]
                        feature_valid = feature_series[valid_mask]
                        
                        # ì •ê·œí™” (StandardScaler: í‰ê·  0, í‘œì¤€í¸ì°¨ 1)
                        scaler = StandardScaler()
                        target_normalized = scaler.fit_transform(target_valid.values.reshape(-1, 1)).flatten()
                        feature_normalized = scaler.fit_transform(feature_valid.values.reshape(-1, 1)).flatten()
                        
                        # ì •ê·œí™”ëœ ë°ì´í„°ë¡œ ìƒê´€ê³„ìˆ˜ ê³„ì‚°
                        corr = np.corrcoef(target_normalized, feature_normalized)[0, 1]
                    else:
                        corr = self._calculate_categorical_correlation(target_series, feature_series)
                    
                    if not np.isnan(corr) and not pd.isna(corr):
                        group_corr[feature] = float(corr)
                except Exception as e:
                    continue
            
            if group_corr:
                group_correlations[str(group_value)] = group_corr
        
        return group_correlations
    
    def _preprocess_column(self, series: pd.Series, group_by: Optional[pd.Series] = None) -> pd.Series:
        """ì»¬ëŸ¼ ì „ì²˜ë¦¬: ë¬¸ìì—´ì„ ìˆ«ìë¡œ ë³€í™˜, ë‚ ì§œëŠ” íƒ€ì„ìŠ¤íƒ¬í”„ ë˜ëŠ” ê·¸ë£¹ë³„ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
        
        Args:
            series: ì „ì²˜ë¦¬í•  ì»¬ëŸ¼ ë°ì´í„°
            group_by: ê·¸ë£¹í™”í•  ê¸°ì¤€ ì»¬ëŸ¼ (ì˜ˆ: ìƒí’ˆ_ID, ìƒí’ˆëª… ë“±). ìˆìœ¼ë©´ ê·¸ë£¹ë³„ë¡œ ë‚ ì§œ ì¸ë±ìŠ¤ ë¶€ì—¬
        """
        # ë‚ ì§œ íƒ€ì…ì¸ì§€ ë¨¼ì € í™•ì¸ (datetime)
        if pd.api.types.is_datetime64_any_dtype(series):
            if group_by is not None:
                # ê·¸ë£¹ë³„ë¡œ ë‚ ì§œë¥¼ ìˆœì„œ ì¸ë±ìŠ¤ë¡œ ë³€í™˜ (ì œí’ˆë³„ë¡œ ë…ë¦½ì ì¸ ì¸ë±ìŠ¤)
                result = pd.Series(index=series.index, dtype=float)
                for group_value in group_by.unique():
                    mask = group_by == group_value
                    group_dates = series[mask].sort_values()
                    date_to_index = {date: idx for idx, date in enumerate(group_dates.unique())}
                    result[mask] = series[mask].map(date_to_index)
                return pd.to_numeric(result, errors='coerce')
            else:
                # ê·¸ë£¹í™” ê¸°ì¤€ì´ ì—†ìœ¼ë©´ íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ë³€í™˜ (ì ˆëŒ€ ì‹œê°„ ìœ ì§€)
                # ë˜ëŠ” ì „ì²´ ìˆœì„œ ì¸ë±ìŠ¤ë¡œ ë³€í™˜ (ì‹œê°„ ì¶”ì„¸ëŠ” ìœ ì§€í•˜ë˜ ì œí’ˆë³„ êµ¬ë¶„ ì—†ìŒ)
                return pd.to_numeric(series, errors='coerce')
        
        # ìˆ«ìí˜•ì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if pd.api.types.is_numeric_dtype(series):
            return pd.to_numeric(series, errors='coerce')
        
        # ë¬¸ìì—´/object íƒ€ì… ì²˜ë¦¬
        try:
            # ìˆ«ìë¡œ ë³€í™˜ ê°€ëŠ¥í•œ ë¬¸ìì—´ì´ë©´ ë³€í™˜ ì‹œë„ (ì˜ˆ: "ë°°ì†¡_ì§€ì—°ì‹œê°„" = "30", "45" ë“±)
            numeric_series = pd.to_numeric(series, errors='coerce')
            if numeric_series.notna().sum() > len(series) * 0.8:  # 80% ì´ìƒì´ ìˆ«ìë©´ ìˆ«ìí˜•ìœ¼ë¡œ ì‚¬ìš©
                return numeric_series
        except:
            pass
        
        # ë²”ì£¼í˜•: ë¼ë²¨ ì¸ì½”ë”©
        le = LabelEncoder()
        encoded = le.fit_transform(series.astype(str).fillna(''))
        return pd.Series(encoded, index=series.index)
    
    def _calculate_categorical_correlation(self, series1: pd.Series, series2: pd.Series) -> float:
        """ë²”ì£¼í˜• ë³€ìˆ˜ ê°„ ìƒê´€ê³„ìˆ˜ ê³„ì‚° (ì›í•« ì¸ì½”ë”© ë˜ëŠ” ë¼ë²¨ ì¸ì½”ë”© ì‚¬ìš©)"""
        
        # í•˜ë‚˜ëŠ” ìˆ«ìí˜•, í•˜ë‚˜ëŠ” ë²”ì£¼í˜•ì¸ ê²½ìš°
        if pd.api.types.is_numeric_dtype(series1) and not pd.api.types.is_numeric_dtype(series2):
            # ë²”ì£¼í˜•ì„ ì›í•« ì¸ì½”ë”© í›„ ê° ë”ë¯¸ ë³€ìˆ˜ì™€ì˜ ìƒê´€ê³„ìˆ˜ ì¤‘ ìµœëŒ€ê°’ ì‚¬ìš©
            dummies = pd.get_dummies(series2, prefix='cat')
            if dummies.empty:
                return 0.0
            # ê° ë”ë¯¸ ë³€ìˆ˜ì™€ì˜ ìƒê´€ê³„ìˆ˜ ì¤‘ ì ˆëŒ“ê°’ì´ ê°€ì¥ í° ê°’
            correlations = [abs(series1.corr(dummies[col])) for col in dummies.columns]
            return max(correlations) if correlations else 0.0
        
        elif not pd.api.types.is_numeric_dtype(series1) and pd.api.types.is_numeric_dtype(series2):
            # ë™ì¼í•˜ê²Œ ì²˜ë¦¬
            dummies = pd.get_dummies(series1, prefix='cat')
            if dummies.empty:
                return 0.0
            correlations = [abs(series2.corr(dummies[col])) for col in dummies.columns]
            return max(correlations) if correlations else 0.0
        
        else:
            # ë‘˜ ë‹¤ ë²”ì£¼í˜•: CramÃ©r's V ë˜ëŠ” ì›í•« ì¸ì½”ë”© í›„ ìƒê´€ê³„ìˆ˜
            try:
                # êµì°¨í‘œ ìƒì„±
                contingency = pd.crosstab(series1, series2)
                # ì¹´ì´ì œê³± ê²€ì •
                chi2, _, _, _ = chi2_contingency(contingency)
                n = contingency.sum().sum()
                # CramÃ©r's V ê³„ì‚°
                cramers_v = np.sqrt(chi2 / (n * (min(contingency.shape) - 1)))
                return float(cramers_v)
            except:
                # ì‹¤íŒ¨ ì‹œ ë¼ë²¨ ì¸ì½”ë”© í›„ í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜
                le1 = LabelEncoder()
                le2 = LabelEncoder()
                encoded1 = pd.Series(le1.fit_transform(series1.astype(str).fillna('')), index=series1.index)
                encoded2 = pd.Series(le2.fit_transform(series2.astype(str).fillna('')), index=series2.index)
                corr = encoded1.corr(encoded2)
                return corr if not pd.isna(corr) else 0.0
    
    async def _create_chart(self, correlations: Dict, target: str) -> str:
        """ì°¨íŠ¸ ìƒì„±"""
        import plotly.graph_objects as go
        import base64
        
        features = list(correlations.keys())
        values = list(correlations.values())
        
        fig = go.Figure(data=[
            go.Bar(x=features, y=values, marker_color='lightblue')
        ])
        fig.update_layout(
            title=f"{target}ê³¼ì˜ ìƒê´€ê´€ê³„",
            xaxis_title="í”¼ì²˜",
            yaxis_title="ìƒê´€ê³„ìˆ˜"
        )
        
        img_bytes = fig.to_image(format="png")
        img_base64 = base64.b64encode(img_bytes).decode('utf-8')
        return img_base64
    
    def _get_top_correlations(self, correlations: Dict, top_n: int = 5) -> List[TopCorrelationItem]:
        """ìƒìœ„ ìƒê´€ê´€ê³„ ì¶”ì¶œ"""
        sorted_items = sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True)
        return [
            TopCorrelationItem(feature=k, correlation=float(v))
            for k, v in sorted_items[:top_n]
        ]

